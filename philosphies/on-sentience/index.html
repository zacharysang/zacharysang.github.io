<!DOCTYPE html><html lang="en-gb"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>On Sentience and Substituion - Zak Sang</title><meta name="description" content="Point 1 - On Thessian Spectrum of Consciousness Argument I am aware of: In particular on AI/AGI sentience, thought experiment is put forward that AGI sentience must be feasible because human cybernetic migration would result in a sentient being. That is, if a human can&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://zacharysang.com/philosphies/on-sentience/"><link rel="alternate" type="application/atom+xml" href="https://zacharysang.com/feed.xml" title="Zak Sang - RSS"><link rel="alternate" type="application/json" href="https://zacharysang.com/feed.json" title="Zak Sang - JSON"><meta property="og:title" content="On Sentience and Substituion"><meta property="og:site_name" content="Zak Sang"><meta property="og:description" content="Point 1 - On Thessian Spectrum of Consciousness Argument I am aware of: In particular on AI/AGI sentience, thought experiment is put forward that AGI sentience must be feasible because human cybernetic migration would result in a sentient being. That is, if a human can&hellip;"><meta property="og:url" content="https://zacharysang.com/on-sentience/"><meta property="og:type" content="article"><link rel="stylesheet" href="https://zacharysang.com/assets/css/style.css?v=11bee19e86e8eda2cf9a60efd975666d"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://zacharysang.com/philosphies/on-sentience/"},"headline":"On Sentience and Substituion","datePublished":"2025-07-19T15:37-07:00","dateModified":"2025-07-19T19:10-07:00","description":"Point 1 - On Thessian Spectrum of Consciousness Argument I am aware of: In particular on AI/AGI sentience, thought experiment is put forward that AGI sentience must be feasible because human cybernetic migration would result in a sentient being. That is, if a human can&hellip;","author":{"@type":"Person","name":"zak","url":"https://zacharysang.com/authors/zak/"},"publisher":{"@type":"Organization","name":"zak"}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body class="page-template"><header class="top js-header"><a class="logo" href="https://zacharysang.com/">Zak Sang</a><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu" aria-haspopup="true" aria-expanded="false"><span class="navbar__toggle-box"><span class="navbar__toggle-inner">Menu</span></span></button><ul class="navbar__menu"><li><a href="https://zacharysang.com/" target="_self">Home</a></li><li class="active-parent has-submenu"><span class="is-separator" title="Philosophies" aria-haspopup="true">Philosophies</span><ul class="navbar__submenu level-2" aria-hidden="true"><li><a href="https://zacharysang.com/philosphies/on-building/" title="On Building" target="_self">On Building</a></li><li><a href="https://zacharysang.com/philosphies/on-communicating/" title="On Communicating" target="_self">On Communicating</a></li><li><a href="https://zacharysang.com/philosphies/on-documents/" target="_self">On Documents</a></li><li><a href="https://zacharysang.com/philosphies/on-organizing/" title="On Organizing" target="_self">On Organizing</a></li><li class="active"><a href="https://zacharysang.com/philosphies/on-sentience/" target="_blank">On Sentience and Substitution</a></li></ul></li></ul></nav></header><main class="page"><article class="content"><div class="hero hero--noimage"><header class="hero__content"><div class="wrapper"><h1>On Sentience and Substituion</h1></div></header></div><div class="entry-wrapper content__entry"><h1>Point 1 - On Thessian Spectrum of Consciousness</h1><ul><li>Argument I am aware of: <em>In particular on AI/AGI sentience, thought experiment is put forward that AGI sentience must be feasible because human cybernetic migration would result in a sentient being. That is, if a human can gradually replace each part of their brain with non-organic components and result in still being sentient after fully replaced. Such implies that a being made from the same components from the beginning must also be sentient</em><ul><li>Supporting this is the claim that the human's sentience does not change throughout this setup process</li><li>Supporting this is the claim that we cannot provide a valid description of reduced sentience in each step of component replacement in the human</li></ul></li><li>My position is that at the very least this line of reasoning is flawed<ul><li>In the empiricist tabula rasa perspective, </li></ul></li></ul><h1>Point 2 - On Decomposability of Consciousness as an Approach to Substitution/ Simulation</h1><ul><li>Argument I am aware of:<ul><li>Electronic digital hardware can perform equivalent computation to neurons and so that implies that it is feasible to replace a full human brain by replacing every neuron with an electronic equivalent. This results in an equivalently sentient brain because the components are equivalent</li></ul></li><li>My position is that at the very least that this makes some false assumptions:<br><ul><li>While neurons are similar to electronic digital components, there are known dissimilarities too (ref, roboticist describes additional features)</li><li>Even if these additional features are also simulated, I expect the underlying physical form to expose more properties at different scales (ie: we may view a given electronic component as equivalent to a neuron at a given scale, but I feel this is unlikely to hold at arbitratry size and time scales.</li><li>My position is that the exercise of substitution on this approach is to additively select observed features of the brain from the bottom up, and that this is flawed because bottom up approaches miss emergent properties at different scales which lead to incomplete simulations.</li><li>Additionally this approach is subject to the exploding magnitude involved not only with the count of components, but the number of possible interactions across complex dimensions.</li><li>To enumerate the complexities<ul><li>Time scales:<ul><li>How does the biological component evolve over time and exist at different time scales as compared to the digital component? Are these the same? These are at least not the same in the sense that excluding certain properties seen as deficiencies or degradations are the driver for substitution in the first place.</li><li>In this approach we are playing the game of selecting properties of the biological neurological brain to populate in different time scales including defining how properties evolve through the transition across timescales. Given that, we cannot generally maintain the assumption that equivalent components will result in an equivalent outcome if we are introducing divergence at different time scales. We cannot assume these alterations have a null effect on the final simulated result</li></ul></li><li>Size scales:<ul><li>Is a component of the brain (eg: a lobe) equivalent to an equivalent structure based on electronic digital neurons? To what extent is the behavior of the whole influenced by the physical form of the parts? To what extent can we rule out emergent properties based on underlying physical properties?</li><li>Equivalence is based on the smallest unit where emergent properties can be known not to exist, which in general is the smallest unit that is indistinguishable from the simulated substitute. This is because emergent properties can arise from the multiplication of any subtle physical property over a larger scale.</li><li>Even if complete equivalence is not the goal, equivalence of any particular property, such as sentience or the supporting properties of sentience, is subject to the above</li><li>If it can be known that either (a) a given property does not result in any emergent properties at larger scales, or that (b) a given property does not influence any emergent properties required for the target property of sentience, then in that case the physical property and the corresponding component can be substituted</li><li>But given that, following the approach of substitution would require either (a) ruling out all emergent properties or (b) identifying the specific properties involved in sentience</li><li>I hold the position that both of the above are not possible</li></ul></li><li>Permutation of interactions across scales: <ul><li>A complete simulation will produce an equivalent outcome for each interaction that occurs within the subject. As a prerequisite to achieving this, in the additive approach, we would need to enumerate the interactions we seek parity on</li><li>The number of relevant scales can be defined by the distance between each scale in scope. The distance between each scale in scope is the size of the smallest unit - that is, the addition of a single unit has the potential to introduce emergent properties as enter us into a new canonical scale.<ul><li>After we enumerate the scales that are relevant (equivalent to the number of indistinguishable units making up the whole in the case that no intermediate component provide the indistinguishable equivlance property), we must also permute since interactions can occur across any arbitrary pair of scales - it is not generally the case that the participants in a given interaction are bound by some distance between their time scales. (though, scale separation theory provides some guidelines where this may be untrue to an extent)</li></ul></li></ul></li></ul></li><li>The counter to all of this is that the goal of neuron substitution is not to create an exact perfect simulation of the brain, but if it is the case that we are targeting a different whole, why should we assume that the property of sentience would be preserved just because a low-level component is substituted with an 'equivalent'?</li></ul></li></ul><h1>Point 3 - On the Quantification of Sentience</h1><ul><li>Argument that I am aware of : Three chickens to one human means killing three chickens is equivalent to killing one human - does any such formula exist?</li><li>My position is that this is nonsensical since a quantified unit implies (a) an atomic kernel (eg: the smallest unit of sentience), (b) regularity as the unit is multiplied (eg: 2 units of sentience = twice as sentient)</li><li>But instead while we have a concept of sentience and a vague agreement that sentience may be a spectrum, we have nowhere close to a specific atomic kernel and I do not believe that multiplication of that kernel would result in predictable increases in observed sentience.</li></ul></div></article></main><footer class="footer"><div class="wrapper"><div class="footer__copyright"><p>Powered by Publii</p></div><button id="backToTop" class="footer__bttop" aria-label="Back to top" title="Back to top"><svg width="20" height="20"><use xlink:href="https://zacharysang.com/assets/svg/svg-map.svg#toparrow"/></svg></button></div></footer><script defer="defer" src="https://zacharysang.com/assets/js/scripts.min.js?v=700105c316933a8202041b6415abb233"></script><script>window.publiiThemeMenuConfig={mobileMenuMode:'sidebar',animationSpeed:300,submenuWidth: 'auto',doubleClickTime:500,mobileMenuExpandableSubmenus:true,relatedContainerForOverlayMenuSelector:'.top'};</script><script>var images = document.querySelectorAll('img[loading]');
        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>